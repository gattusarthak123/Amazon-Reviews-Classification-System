
\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}




\newpage
\tableofcontents
\newpage
%\setcounter{chapter}{1}
\section{Introduction}
{\large The reviews are particularly used so that new users can see the rated comments in order to help them make their own purchasing decisions. The quality of the comments made on internet forums has always suffered due to it's users. Different websites have tried different methods for extracting more useful comments so that user can see top rated comments.\\ \\Amazon’s system, in particular, then allows for the higher rated comments to be displayed at the top of the review forum.Even though it is not effective as poor quality reviews can still be seen on top .As a major reason that people are willing to buy consumer goods online without seeing the items themselves, is that they have access to others peoples opinions of the item to buy the product. This effects the sale on Amazon.\\ \newline The reason for the failure is part of the algorithm for determining the order of the reviews relies on how recently the review has been made.The system should predict that the comment is helpful or not so that poor quality reviews should not be at the top.}\\ \\ \\
\section{Problem Statement}
{\large The problem being addressed in this project is the poor quality of Amazon reviews at the top of the forum despite the helpfulness rating system. The solution this problem is to pre-rate the reviews as "helpful" or "not helpful" for new reviews and the most helpful reviews are given position at the top.This way, poor quality reviews will be more unlikely to be shown at the top.\\ \\This is a binary classification of reviews on the basis of helpfulness as helpful or not helpful.The models will be trained on the basis of features extracted from the reivews.The recent reviews which are not rated as helpful by the users will be of poor quality but are still on top, we tried to make the helpful review to be at the top of the reviews shown to the buyer. \\ }
\newpage
\section{Methodology}

\subsection{Data Exploration}
 {\large We converted the given in "json" to a datframe in order to perform our analysis.\\ \\The data looks like this,\\
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{"pic1"}
	\caption{The Dataset}
	\label{fig:screenshot-1}
\end{figure}


{\large The data contain ratings for each products from the users rated from one to five based on their experience.}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{pic2}
	\caption{Frequency of rating}
	\label{fig:one}
\end{figure}


The user can either rate the review as 'helpful’ or ‘not helpful’. The dataset records each of these in an array. For our problem we want to classify the new review as either ‘helpful’ or not ‘helpful’. For training, this label can be generated by dividing the ‘helpful’ ratings by the total ratings and seeing if it exceeds a certain threshold.\\ \\
The 'reviewText' will be used to generate features using natural language processing.\\
The 'helpful' rating will be used to generate labels. We will train our model using these training labels.\\ Predict the label using the test features, and measure the success of our model using the test labels.\\



\begin{figure}[h]
	\centering
	\includegraphics[width=01.0\linewidth]{pic3}
	\caption{Splitting helpful array}
	\label{fig:capture2}
\end{figure}
We selected the required columns which includes 'ReviewText', 'helpful numerator','overall rating' and 'helpful denominator'.}\\ 


{\large Now lets do a visualization and a count of the data in order to get a sense of the correlation and distribution.}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{pic4}
	\caption{Correlation}
	\label{fig:capture6}
\end{figure}

\subsection{Data Preprocessing}

{\large The product which has less than 25 ratings are trimmed from the dataset.As some good reviews which are recent which are not marked as helpful or not helpful will be rated as not helful. }\\
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{pic5}
	\caption{Data Selection}
	\label{fig:capture3}
\end{figure}\\
{\large To determine whether the review is helpful or not.TFor this we use a threshold of 'helpful numerator' divided by 'helpful denominator'.If this ratio exceeds a certain threshold value, we can label the training data as 'helpful' = 1, or 'non-helpful' = 0. For this analysis, the threshold is set to 0.5.}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{pic6}
	\caption{Classification on basis of helpfulness}
	\label{fig:capture4}
	
\end{figure}\\
 

{\large The preprocessing of reviewtext is performed by:
\begin{itemize}
	\item Stemming-The suffix of words are taken out.
	\item Tolkenizing- Splits sentences up into single words.
	\item Remove Stop Word-This moves words such as 'the' 'a' and 'it'.
	\item ngrams- Makes groups of words that are 'n' long.
\end{itemize}
Finally we will generate TF-IDF scores for each of the stemmed and tolkenized words and ngrams. TF-IDF is short for Term Frequency Inverse Document frequency. TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.}


\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{pic8}
	\caption{Review TF-IDF score}
	\label{fig:capture7}
\end{figure}
\subsection{Benchmark model}
{\large
The following algorithms were used:
\begin{itemize}
\item	Logistic Regression.
\item	GaussianNB.
\item	AdaBoostClassifie.
\item	RandomForestClassifier.
\item	DecisionTreeClassifier.\\ \\
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{pic9}
	\caption{Test and Train split}
	\label{fig:capture}
\end{figure}
{\large Now we will split the data into 80 percent training and 20 percent testing.\\
	The benefit to splitting the data into testing and training sets is that this allows simulated evaluation of how well the model is performing before using it in the real world to make predictions.}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{pic7}
	\caption{Model Selection}
	\label{fig:capture5}
\end{figure}


\textbf{Logistic Regression}: Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier.In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.72\linewidth]{picnew1}
	\caption{Sigmoid Activation Funtion\cite{5}}
	\label{fig:captur}
\end{figure}
\\If ‘Z’ goes to infinity, Y(predicted) will become 1 and if ‘Z’ goes to negative infinity, Y(predicted) will become 0.
\newpage
\textbf{Random Forest}: A Random Forest is a ensembling algorithm that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. It does not implement boosting on the trees, and instead counts a vote of each individual tree in order to produce the final class label.\\ \\
It is a statistical algorithm that is used to cluster points of data in functional groups. When the data set is large and/or there are many variables it becomes difficult to cluster the data because not all variables can be taken into account, therefore the algorithm can also give a certain chance that a data point belongs in a certain group.\\ \\
Its has a variety of applications, such as recommendation engines, image classification and feature selection. It can be used to classify loyal loan applicants, identify fraudulent activity and predict diseases. It lies at the base of the Boruta algorithm, which selects important features in a dataset.\\


\begin{figure}[h]
	\centering
	\includegraphics[width=0.72\linewidth]{picnew5}
	\caption{Random Forest Simplified\cite{5}}
	\label{fig:captur}
\end{figure}
\newpage
\textbf{Adaboost}: AdaBoost or "adaptive boosting" begins by fitting a "weak" classifier on the original dataset. It then fits additional copies of the classifier on the same dataset and adjusts the weights of incorrectly classified instances such that subsequent classifiers focus more on difficult cases.\\ \\AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier.\\ \\ The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as base classifier if it accepts weights on the training set. Adaboost should meet two bullet point conditions:
\begin{itemize}
	\item	The classifier should be trained interactively on various weighed training examples.
	\item	In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{picnew4}
	\caption{AdaBoost classifier\cite{5}}
	\label{fig:capture65}
\end{figure}
\newpage
\textbf{GaussianNB}:Gaussian Naive Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms. Naive Bayes classifier is the fast, accurate and reliable algorithm. Naive Bayes classifiers have high accuracy and speed on large datasets.\\
\begin{equation}
%\label{eq1}
P(\frac{h}{D})=\frac{P(D|h)P(h)}{P(D)}
\end{equation} 
\begin{itemize}
	\item	P(h): the probability of hypothesis h being true (regardless of the data). This is known as the prior probability of h.
	\item	P(D): the probability of the data (regardless of the hypothesis). This is known as the prior probability.
	\item	P(h|D): the probability of hypothesis h given the data D. This is known as posterior probability.
	\item	P(D|h): the probability of data d given that the hypothesis h was true. This is known as posterior probability.
\end{itemize}
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=1\linewidth]{picnew3}
%	\caption{Gaussian Nave Bayes}
%	\label{fig:captur}
%\end{figure}
\newpage
\textbf{Decision Tree}:A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{picnew22}
	\caption{Sample example(image) of Decision\cite{5}}
	\label{fig:captur}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{picnew2}
	\caption{Sample example(image) of Decision\cite{5}}
	\label{fig:captur}
\end{figure}
\newpage
\section{ Simulations and Results}
We tested the above classifiers using above function.All of the classifiers from sklearn will be tested, We will train and test a bunch of the classifiers on four different training sizes in order to find which one will be our benchmark.
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{pic10}
	\caption{Training and Testing}
	\label{fig:capture8}
\end{figure}

The results obtained are as follows:
\begin{table}[h]
	\centering
	\caption{Model and Training set size accuracy table}
	\begin{tabular}{l|c|c|c}
		\hline
		Model&Training Set Size&Train Set Score&Test Set Score \\
		\hline
		GaussianNB&500&0.9958&0.4908\\
		\hline
		&800&0.9934&0.4894\\
		\hline
		&1400&0.9906&0.5106\\
		\hline
		&1590&0.9888&0.5330\\
		\hline
		AdaBoostClassifier &500&1.0000&0.6894\\
		\hline
	 &800&1.0000&0.6703\\
		\hline
		 &1400&0.9981&0.7588\\
		\hline
		 &1590&0.9933&0.7536\\
		\hline
		RandomForestClassifier &500&1.0000&0.5542\\
		\hline
		 &800&1.0000&0.5858\\
		\hline
		 &1400&1.0000&0.5477\\
		\hline
		 &1590&1.0000&0.6147\\
		\hline
		LogisticRegression &500&1.0000&0.8285\\
		\hline
		 &800&.9984&0.8509\\
		\hline
		&1400&0.9973&0.8281\\
		\hline
		 &1590&0.9967&0.8533\\
		\hline
		DecisionTreeClassifier &500&1.0000&0.5420\\
		\hline
		&800&1.0000&0.5420\\
		\hline
		 &1400&1.0000&0.5460\\
		\hline
		 &1590&1.0000&0.5763\\
		\hline
	\end{tabular}
\end{table}

{\large We found that Logistic Regression classifier did the best and most suitable with maximum testing set score as 0.8533 and GaussianNb is least suitable with test set score 0.5330}
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{pic11}
	\caption{Comparison of Different Classifiers}
	\label{fig:capture9}
\end{figure}
{\large The accuracy score for test set for four different sizes of train dataset is plotted for all five models which shows Logistic Regression as the most suitable and GaussianNB as least suitable.}\\
\section{ Conclusions and Future Work }
Our model takes in existing 'reviewText' and transforms it into numerical TF-IDF scores.It then adds the existing 'overall' score of the reviews to create a features set for each review. It trains a Logistic Regression model using labels generated by taking existing 'helpfulness numerator' data and dividing it by 'helpfulness denominator' data and thresholding the result at 0.5. The products which have less than 25 reviews are not considered for training the classifier.  New reviews with no helpfulness data which are recently added are classified as being 'helpful' or 'non-helpful'. By using this system, Amazon can work to make sure that more helpful reviews are shown at the top of their forums.\\ \\Mainly, we looked at the TFIDF features generated from amazon review text and added the 'overall rating' that was given to the product by the reviewer. We used these features to predict how 'helpful' other users would find the review.
\newpage
In order to improve the accuracy of our model, the following actions could be taken:\\
\begin{itemize}
	\item There are lots of spelling mistakes in the reviews and abbrevations are used. This would result in a model that potentially has less features, as certain spelling errors would have been corrected/eliminated.
	\item Reviews with poor grammar, punctuation and improper word endings are more difficult to understand and would possibly lead to people rating them as less helpful.These reviews may contain useful informations and may be helpful but certain other factors made them rated as not helpful.Thus preprocessing of data can be improved either by varying the values of threshold or the reviews that has not been selected because the product is not much rated.
\end{itemize}
\newpage
\begin{thebibliography}{1}
	\bibitem{1}Liaw A. Weiner M., Classification and Regression by randomForest. R News. 2002;Vol 2(2):18-22.
	\bibitem{2}J. McAuley, R. Pandey, J. Leskovec Knowledge Discovery and Data Mining, 2015.
	\bibitem{3}HUI BWU Y. Anti-spam model based on semi-Naive Bayesian classification model. Journal of Computer Applications. 2009;29(3):903-904.
	\bibitem{4}Quinlan,  J.  R.  (1986). Induction  of  decision  trees.  Machine  learning, 1(1), 81-106. 
	\bibitem{5} https://drrajeshkumar.wordpress.com/downloads/
\end{thebibliography}
\nocite{*}
\bibliographystyle{unsrt}

\end{document}
